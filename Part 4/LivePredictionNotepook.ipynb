{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Video Capture and CNN Process from Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### taken from https://medium.com/@neotheicebird/webcam-based-image-processing-in-ipython-notebooks-47c75a022514"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  ASSUMES: you have trained CNN and you have Saved the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1:  Installing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "running_env = \"jupyter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (4.1.1.26)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from opencv-python) (1.17.4)\n",
      "Requirement already satisfied: matplotlib in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (3.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.8.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.17.4)\n",
      "Requirement already satisfied: six in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (41.4.0)\n",
      "Requirement already satisfied: tqdm in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (4.36.1)\n",
      "^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.11.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.17.4)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from scikit-learn) (0.13.2)\n",
      "Requirement already satisfied: tensorflow==2.0.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (2.0.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (3.10.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (0.33.6)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.0.8)\n",
      "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (0.8.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (3.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (0.8.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (0.1.7)\n",
      "Requirement already satisfied: six>=1.10.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.12.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.24.3)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.1.0)\n",
      "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (2.0.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.11.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (0.2.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorflow==2.0.0) (1.17.4)\n",
      "Requirement already satisfied: setuptools in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from protobuf>=3.6.1->tensorflow==2.0.0) (41.4.0)\n",
      "Requirement already satisfied: h5py in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.9.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.6.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.16.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.2.7)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (4.0)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.4.7)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.22.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/RayM/opt/anaconda3/lib/python3.7/site-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (2019.9.11)\n"
     ]
    }
   ],
   "source": [
    "# STEP 1:  Install packages in the current environment\n",
    "# STEP 1:  Install packages in the current environment\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if(running_env == \"jupyter\"):\n",
    "    !{sys.executable} -m pip install opencv-python \n",
    "    !{sys.executable} -m pip install matplotlib\n",
    "    !{sys.executable} -m pip install tqdm\n",
    "    !{sys.executable} -m pip install scikit-learn\n",
    "    !{sys.executable} -m pip install tensorflow==2.0.0\n",
    "else:\n",
    "    #Python Import Statements\n",
    "    #os.system.executable(\"opencv-python\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2:  Importing our dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "#from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import glob\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from IPython import display\n",
    "import logging\n",
    "#see for logging https://docs.python.org/2/howto/logging.html#logging-basic-tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.enable_eager_execution()\n",
    "#tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "IMG_SHAPE  = 224  # size of our input image needed for our model IMG_SHAPE x IMG_SHAPE x 3 (color)\n",
    "LOG_TO_FILE = 0\n",
    "VERBOSE = 1\n",
    "BASE_DIRECTORY ='/Users/RayM/Documents/School/CompVision/LIVE_VIDEO/'\n",
    "LABELS = ['CellLeader','ColumnFormation','Come','CoverThisArea','Crouch','Door','Eight','FileFormation','Five','Four','Freeze','Gas','Hostage','HurryUp','IDontUnderstand','IUnderstand','LineAbreastFormation','Listen','Me','Nine','One','Pistol','PointOfEntry','RallyPoint','Rifle','Seven','Shotgun','Six','Sniper','Stop','Ten','Three','Two','Vehicle','Watch','Window','You']\n",
    "#Labels for the Activity Recognition\n",
    "#LABELS = ['UnevenBars','ApplyLipstick','TableTennisShot','Fencing','Mixing','SumoWrestling','HulaHoop','PommelHorse','HorseRiding','SkyDiving','BenchPress','GolfSwing','HeadMassage','FrontCrawl','Haircut','HandstandWalking','Skiing','PlayingDaf','PlayingSitar','FrisbeeCatch','CliffDiving','BoxingSpeedBag','Kayaking','Rafting','WritingOnBoard','VolleyballSpiking','Archery','MoppingFloor','JumpRope','Lunges','BasketballDunk','Surfing','SkateBoarding','FloorGymnastics','Billiards','CuttingInKitchen','BlowingCandles','PlayingCello','JugglingBalls','Drumming','ThrowDiscus','BaseballPitch','SoccerPenalty','Hammering','BodyWeightSquats','SoccerJuggling','CricketShot','BandMarching','PlayingPiano','BreastStroke','ApplyEyeMakeup','HighJump','IceDancing','HandstandPushups','RockClimbingIndoor','HammerThrow','WallPushups','RopeClimbing','Basketball','Shotput','Nunchucks','WalkingWithDog','PlayingFlute','PlayingDhol','PullUps','CricketBowling','BabyCrawling','Diving','TaiChi','YoYo','BlowDryHair','PushUps','ShavingBeard','Knitting','HorseRace','TrampolineJumping','Typing','Bowling','CleanAndJerk','MilitaryParade','FieldHockeyPenalty','PlayingViolin','Skijet','PizzaTossing','LongJump','PlayingTabla','PlayingGuitar','BrushingTeeth','PoleVault','Punch','ParallelBars','Biking','BalanceBeam','Swing','JavelinThrow','Rowing','StillRings','SalsaSpin','TennisSwing','JumpingJack','BoxingPunchingBag']\n",
    "#DECISION_DIFFERENCE_THRESHOLD = 0.2 # the higher decision value must be greater than other classes by this much\n",
    "DECISION_DIFFERENCE_THRESHOLD = 0.003056 # Arbitrarily calculated based on size of dataset\n",
    "DECISION_DIFFERENCE_THRESHOLD = 0.0001980198 #For Activity Recognizer\n",
    "SEQUENCE_LENGTH = 41\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:root:Starting Porgram\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0\n"
     ]
    }
   ],
   "source": [
    "#print out version of tensorflow using\n",
    "print(tf.version.VERSION)\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "#setup logging so will also go to file mylog.log\n",
    "import time\n",
    "\n",
    "if(LOG_TO_FILE == 1):\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    file_log = 'log'+ timestr +'.log'    #will give unique timestamp to log file\n",
    "    fhandler = logging.FileHandler(filename=file_log, mode='a')\n",
    "    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "    fhandler.setFormatter(formatter)\n",
    "    logger.addHandler(fhandler)\n",
    "\n",
    "#start logging  (options see https://docs.python.org/2/howto/logging.html#logging-basic-tutorial )  have: .info, .warning\n",
    "logging.debug(\"Starting Porgram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: Define methods to help you a= load you model  b= process image so is ready for input to model c= call prediciton on model using appropropriately processed image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.a functions dealing with loading of pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to load from h5 file\n",
    "def  loadModelFrom_H5_File(model_file):\n",
    "\n",
    "    # try to reload the saved h5 file\n",
    "    # Recreate the exact same model, including its weights and the optimizer\n",
    "    new_model = tf.keras.models.load_model(model_file)\n",
    "\n",
    "    # Show the model architecture\n",
    "    new_model.summary()\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method to load from SavedModel directory\n",
    "def loadModelFrom_SavedModel_Directory(saved_model_dir):\n",
    " \n",
    "    print(\"Loading model from saved model at: \" + str(saved_model_dir))\n",
    "\n",
    "    #new_modelw = tf.keras.models.load_model(saved_model_dir)\n",
    "    new_modelw = tf.keras.experimental.load_from_saved_model(saved_model_dir)\n",
    "\n",
    "    # Check its architecture\n",
    "    new_modelw.summary()\n",
    "    return new_modelw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.b functions dealing with preprocessing of image for presentation to model input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resizeImageForModel(image):\n",
    "    \n",
    "    \n",
    "    #resize the image to IMG_SHAPExIMG_SHAPE\n",
    "    #   note alternative to specify method of resizing- image = cv2.resize(frame, dim, interpolation = cv2.INTER_AREA)\n",
    "    imageNew = cv2.resize(frame, (IMG_SHAPE,IMG_SHAPE))\n",
    "\n",
    "    return imageNew\n",
    "\n",
    "\n",
    "#function to resize AND rescale image values\n",
    "def preProcessImage(image):\n",
    "    \n",
    "    logging.info(\"preProcessImage: original size \" + str(image.shape))\n",
    "    #change size if needed\n",
    "    newimg = cv2.resize(image,(IMG_SHAPE, IMG_SHAPE))\n",
    "    logging.info(\"preProcessImage: new size \" + str(newimg.shape))\n",
    "    \n",
    "    \n",
    "    #now rescale the image as in training of model are using ImageDataGenerator with rescale=1./255\n",
    "    newimg = np.array(newimg).astype('float32')/255\n",
    "    logging.info(\"preProcess:  rescaling values /255\")\n",
    "    if(VERBOSE==1):\n",
    "        print(newimg)\n",
    "        \n",
    "        \n",
    "\n",
    "    return newimg\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create MobileNetV2 feature extraction model\n",
    "mobilenet_v2 = tf.keras.applications.mobilenet_v2.MobileNetV2(input_shape=(224,224,3), include_top=False, weights='imagenet')\n",
    "x = mobilenet_v2.output\n",
    "\n",
    "pooling_output = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "feature_extraction_model = tf.keras.Model(mobilenet_v2.input,pooling_output)\n",
    "\n",
    "def dataset_feature_extraction(passed_image):\n",
    "    #Convert the dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(passed_image)\n",
    "    \n",
    "    # Extract the features\n",
    "    all_features = []\n",
    "    \n",
    "    # go through the dataset and use the mobilenet_v2 model to \n",
    "    # extract the features for each frame\n",
    "    counter = 0\n",
    "    for img in tqdm.tqdm(dataset):\n",
    "        counter += 1\n",
    "        #print(\"ON image \" + str(counter))\n",
    "        batch_features = feature_extraction_model(img)\n",
    "        # reshape the tensor\n",
    "        batch_features = tf.reshape(batch_features,\n",
    "                                    (batch_features.shape[0], -1))\n",
    "\n",
    "        for features in batch_features.numpy():\n",
    "            all_features.append(features)\n",
    "            \n",
    "    \n",
    "    #We can try just returning  this for now.  May have to add garbage labels later\n",
    "    #return padded_sequence\n",
    "    print(\"returning all features\")\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3.c functions dealing with invoking prediction of model using an apprpropriately process image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# passing input = processed video featureset (in npy?) , model = loaded model\n",
    "def  predict(input, model):\n",
    "    #setup input tensor  of shape (batch=1, height, width, channels) = (1, 150,150,3)\n",
    "    tensor_input = np.expand_dims(input,axis=0)\n",
    "    print(\"tensor shape is \" + str(tensor_input.shape))\n",
    "    if(VERBOSE==1):\n",
    "        print(tensor_input)\n",
    "\n",
    "\n",
    "    # Generate predictions (probabilities -- the output of the last layer)\n",
    "    # on new data using `predict`\n",
    "    print('\\n# Generate predictions ')\n",
    "    #predictions = model.predict_generator(val_data_gen, verbose=1 )\n",
    "    prediction = model.predict(tensor_input, batch_size=1, verbose=VERBOSE)\n",
    "    print(prediction)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to MAKE decision and return a label\n",
    "def makeDecision(predictions):\n",
    "    predictions2 = list(predictions)\n",
    "    max_in_set = max(predictions2)\n",
    "    #print(\"Max:\" + max_in_set)\n",
    "    #Cast to list to make remove work\n",
    "    predictionsList = predictions2[:]\n",
    "    predictionsList.remove(max_in_set)\n",
    "    #print(\"NEw list: \")\n",
    "    #print(predictionsList)\n",
    "    max_in_set2 = max(predictionsList)\n",
    "    if(max_in_set >= max_in_set2 and (max_in_set-max_in_set2) > DECISION_DIFFERENCE_THRESHOLD):\n",
    "        label = LABELS[predictions2.index(max_in_set)]\n",
    "    else:\n",
    "        label = \"Unknown\"\n",
    "    \n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: Load the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Sequential models without an `input_shape` passed to the first layer cannot reload their optimizer state. As a result, your model isstarting with a freshly initialized optimizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "masking (Masking)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  3672064   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  131328    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  9509      \n",
      "=================================================================\n",
      "Total params: 3,812,901\n",
      "Trainable params: 3,812,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## step 4.1 call one of the previous methods to load model\n",
    "\n",
    "model_file = os.path.join(BASE_DIRECTORY, 'TF1-14_LSTM_Model_V1.h5')\n",
    "\n",
    "model = loadModelFrom_H5_File(model_file)   \n",
    "\n",
    "\n",
    "#alternative  loadModeFrom_SavedModel_Directory\n",
    "#saved_model_dir = os.path.join(BASE_DIRECTORY, 'saved_model\\catsdogsCNN')\n",
    "#model = loadMOdelFrom_SavedModel_Directory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: function to store current frame to file and also, list its name with decision and prediction vector in a special prediction_result file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stores the OpenCV2 image to unique file in directory and adds labe,prediction array, imageFileName to the results\n",
    "# file indicated by the file_handle\n",
    "def storeLivePredictions(label, predictions, images, file_handle):\n",
    "    print(\"label \"+ label)\n",
    "    #print(\"predictions[0] \" + str(predictions[0]))\n",
    "    #print(\"predictions[1] \" + str(predictions[1]))\n",
    "    #print(\"directory \" + directory)\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    #my_absolute_dirpath = os.path.abspath(os.path.dirname(__file__))\n",
    "    #print(\"DIRPATH: \" + my_absolute_dirpath)\n",
    "    directory = \"/Users/RayM/Documents/School/CompVision/LIVE_VIDEO/results/\" + label + \"_live_\" + timestr\n",
    "    print(\"directory \" + directory)\n",
    "    \n",
    "    #create the image filename uniquely with timestamp\n",
    "    #timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    os.mkdir(directory)\n",
    "\n",
    "    for i in range(0, len(images)-1):\n",
    "        filename= \"img_\" + str(i)  +\".jpg\"\n",
    "        image_filename = os.path.join(directory, filename)    \n",
    "        #store image to file\n",
    "        print(\"writing image to \" + image_filename)\n",
    "        #print(images[i])\n",
    "        print(\"Shape: \")\n",
    "        print(np.array(images[i]).shape)\n",
    "        if not cv2.imwrite(image_filename, images[i]):\n",
    "            raise Exception(\"Could not write image \" + str(i))\n",
    "        #cv2.imwrite(image_filename, images[i])\n",
    "\n",
    "    #store results to the results file in file_handle\n",
    "    file_handle.write(\"\\n Label: %s  Predictions: [\" %  label)\n",
    "    for item in predictions:\n",
    "        file_handle.write(\"%s \" % item)\n",
    "    file_handle.write(\"]\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results Directory (where will store images) /Users/RayM/Documents/School/CompVision/LIVE_VIDEO/results\n",
      "Results Filename (text info/decisions) /Users/RayM/Documents/School/CompVision/LIVE_VIDEO/results/live_results_20191126-190229.txt\n"
     ]
    }
   ],
   "source": [
    "results_directory = os.path.join(BASE_DIRECTORY, 'results')\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "results_filename = 'live_results_' + timestr +'.txt'    #will give unique timestamp to log file\n",
    "results_filename = os.path.join(results_directory, results_filename)\n",
    "\n",
    "#if not os.path.exists(results_filename):\n",
    "#    os.makedirs(results_filename)\n",
    "    \n",
    "results_file_handle = open(results_filename, 'w+')  #open for writing and append otherwise\n",
    "results_file_handle.write(\"NEW RESULTS: \" + timestr + \"\\n\\n\")\n",
    "\n",
    "print(\"Results Directory (where will store images) \" + results_directory)\n",
    "print(\"Results Filename (text info/decisions) \" + results_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6: Live Video Capture Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.1.1) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-de3dece1ab2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# Lookout for a keyboardInterrupt to stop the script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mis_capturing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframeorig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframeorig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# makes the blues image look real colored\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#webcam_preview.set_data(frame)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.1) /Users/travis/build/skvark/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "#url = 'http://192.168.1.71:8081'\n",
    "url = 'http://192.168.0.21:8081'\n",
    "vc = cv2.VideoCapture(url)\n",
    "#vc = cv2.VideoCapture(0)  #opencv handler for capturing video\n",
    "current_frame = 0\n",
    "sample_every_frame = 1 #SAMPLE EVERY FRAME for high-fidelity data\n",
    "max_images = SEQUENCE_LENGTH\n",
    "frames = []\n",
    "frames_orig = []\n",
    "\n",
    "#now = datetime.now()\n",
    "#frameNum = 0\n",
    "\n",
    "\n",
    "if vc.isOpened(): # try to get the first frame\n",
    "    is_capturing, frame = vc.read()    #read frame\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)    # makes the blues image look real colored\n",
    "    #webcam_preview = plt.imshow(frame) \n",
    "    \n",
    "else:\n",
    "    is_capturing = False\n",
    "\n",
    "while is_capturing:\n",
    "    try:    \n",
    "        #frameNum += 1\n",
    "        #later = datetime.now()\n",
    "        #diff = later-now\n",
    "        #diff_in_seconds = diff.days*24*60*60 +  diff.seconds\n",
    "        #print(\" at frame \" + str(frameNum) + \" with diff time\" + str(diff_in_seconds))\n",
    "\n",
    "        #if(diff_in_seconds > 0):\n",
    "        #    fps = frameNum/diff_in_seconds\n",
    "        #    print(\"Frame rate: \" + str(fps) + \" fps\")\n",
    "        \n",
    "        \n",
    "        #STEP 1: Capture the current Frame\n",
    "        # Lookout for a keyboardInterrupt to stop the script\n",
    "        is_capturing, frameorig = vc.read()\n",
    "        frame = cv2.cvtColor(frameorig, cv2.COLOR_BGR2RGB)    # makes the blues image look real colored\n",
    "        \n",
    "        #webcam_preview.set_data(frame)\n",
    "        #webcam_preview = plt.imshow(frame)  \n",
    "        #plt.draw()\n",
    "\n",
    "        #In-line preprocessing\n",
    "        if current_frame % sample_every_frame == 0:\n",
    "            frame = frame[:, :, ::-1]\n",
    "            #sanity check\n",
    "            #img = preProcessImage(frame)\n",
    "            img = tf.image.resize(frame, (224, 224))\n",
    "            img = tf.keras.applications.mobilenet_v2.preprocess_input(img)\n",
    "            img = np.expand_dims(img,axis=0)\n",
    "\n",
    "            max_images -= 1\n",
    "            frames_orig.append(frameorig)\n",
    "            frames.append(img)\n",
    "            \n",
    "        if max_images == 0:\n",
    "            #Send off\n",
    "            #print(\"Frames whole:\")\n",
    "            #print(frames)\n",
    "            feature_set = dataset_feature_extraction(frames)\n",
    "            #Call prediction on the processed image to our model\n",
    "            predictions = predict(feature_set, model)\n",
    "            logging.debug(predictions)\n",
    "            #Make decision\n",
    "            label = makeDecision(predictions[0])\n",
    "            print(label)\n",
    "            #Store results\n",
    "            storeLivePredictions(label, predictions[0], frames_orig, results_file_handle)\n",
    "            #Clear/reset variables\n",
    "            current_frame = 0 #Just to prevent overflow after a while\n",
    "            frames = []\n",
    "            frames_orig = []\n",
    "            max_images = SEQUENCE_LENGTH\n",
    "            \n",
    "        current_frame += 1\n",
    "        display.clear_output(wait=True)\n",
    "        #plt.pause(0.03333333333)    # the pause time is = 1 / framerate\n",
    "        \n",
    "        \n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        vc.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
